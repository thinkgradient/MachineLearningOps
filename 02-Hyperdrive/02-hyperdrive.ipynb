{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Hyperparameter Tuning with `Hyperdrive`\n",
    "\n",
    "The goal of this lab is to show how to utilize AML service feature called *Hyperdrive* for scale out hyper parameter tuning.\n",
    "\n",
    "Azure Machine Learning allows you to automate hyperparameter exploration in an efficient manner, saving you significant time and resources. You specify the range of hyperparameter values and a maximum number of training runs. The system then automatically launches multiple simultaneous runs with different parameter configurations and finds the configuration that results in the best performance, measured by the metric you choose. Poorly performing training runs are automatically early terminated, reducing wastage of compute resources. These resources are instead used to explore other hyperparameter configurations.\n",
    "\n",
    "You will continue working on the same scenario as in Lab 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect AML workspace\n",
    "\n",
    "Check the version of AML SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK Version: 1.0.23\n"
     ]
    }
   ],
   "source": [
    "# Verify AML SDK Installed\n",
    "\n",
    "import azureml.core\n",
    "print(\"SDK Version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: /home/byteb/events/MachineLearningOps/.azureml/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLOpsFatosIsmali\n",
      "DSIMLOpsHack\n",
      "westeurope\n",
      "051aa254-957d-4431-a6df-6caa8963bdd7\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "# Connect to workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Hyperdrive run\n",
    "### Create a training script\n",
    "We will utilize the similar training script to the one used in Lab 1. The only difference is that in addition to fine tuning **C** we will also try to find the most optimal *Logistic Regression* solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './script'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./script/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from azureml.core.run import Run\n",
    "\n",
    "# Retrieve command line arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str,  help='data folder mounting point')\n",
    "parser.add_argument('--filename', type=str,  help='training file name')\n",
    "parser.add_argument('--C', type=float , help='regularization')\n",
    "parser.add_argument('--solver', type=str , help='Algorithm to use int the optimization problem')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Configure a path to training data\n",
    "data_folder = os.path.join(args.data_folder, 'datasets')\n",
    "print('Loading data from: ', data_folder)\n",
    "data_csv_path = os.path.join(data_folder, args.filename)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(data_csv_path)\n",
    "\n",
    "# Preprocess the data\n",
    "feature_columns = [\n",
    "                   # Demographic\n",
    "                   'age', \n",
    "                   'job', \n",
    "                   'education', \n",
    "                   'marital',  \n",
    "                   'housing', \n",
    "                   'loan', \n",
    "                   # Previous campaigns\n",
    "                   'month',\n",
    "                   'campaign',\n",
    "                   'poutcome',\n",
    "                   # Economic indicators\n",
    "                   'emp_var_rate',\n",
    "                   'cons_price_idx',\n",
    "                   'cons_conf_idx',\n",
    "                   'euribor3m',\n",
    "                   'nr_employed']\n",
    "\n",
    "df = df[feature_columns + ['y']]\n",
    "df_train = pd.get_dummies(df, drop_first=True).astype(dtype='float')\n",
    "\n",
    "# Create logistic regression estimater\n",
    "lr = LogisticRegression(solver=args.solver, C=args.C, max_iter=300, class_weight='balanced')\n",
    "\n",
    "# Logistic regression requires feature scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create a training pipeline\n",
    "pipeline = Pipeline(steps=[('scaler', scaler),\n",
    "                           ('lr', lr)])\n",
    "\n",
    "\n",
    "# Train and evaluate the model using cross validation\n",
    "X = df_train.drop('y', axis=1)\n",
    "y = df_train.y\n",
    "\n",
    "# Evaluate metrics(s) by cross-validation\n",
    "print(\"Starting training using {} solver and C={}\".format(args.solver, args.C))\n",
    "scoring = ['accuracy', 'recall']\n",
    "scores = cross_validate(pipeline, X, y, \n",
    "                        cv=10, \n",
    "                        return_train_score=False,\n",
    "                        scoring=scoring)\n",
    "\n",
    "cv_accuracy = np.mean(scores['test_accuracy'])\n",
    "cv_recall = np.mean(scores['test_recall'])\n",
    "\n",
    "print(\"CV accuracy: \", cv_accuracy)\n",
    "print(\"CV recall: \", cv_recall)\n",
    "\n",
    "# Persist the metrics in Azure ML Experiment\n",
    "# Acquire the current run and log run parameters and performance measures\n",
    "run = Run.get_context()\n",
    "run.log(\"Solver\", args.solver)\n",
    "run.log(\"C\", args.C)\n",
    "run.log(\"val_accuracy\", cv_accuracy)\n",
    "run.log(\"val_recall\", cv_recall)\n",
    "\n",
    "\n",
    "# Train the model on a full dataset\n",
    "trained_pipeline = pipeline.fit(X, y)\n",
    "\n",
    "# Serialize the model to ./outputs directory so that it can be automatically copied to Azure ML Experiment\n",
    "print(\"Saving the model to outputs ...\")\n",
    "joblib.dump(value=trained_pipeline, filename='outputs/model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Azure ML Compute\n",
    "\n",
    "We are reusing the cluster created in Lab 1. In case you removed the cluster the below code snippet is going to re-create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target, using this compute target instead of creating:  cpu-cluster\n"
     ]
    }
   ],
   "source": [
    "# Create an Azure ML Compute cluster\n",
    "\n",
    "# Create Azure ML cluster\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"cpu-cluster\"\n",
    "cluster_min_nodes = 1\n",
    "cluster_max_nodes = 3\n",
    "vm_size = \"STANDARD_DS11_V2\"\n",
    "\n",
    "# Check if the cluster exists. If yes connect to it\n",
    "if cluster_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[cluster_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('Found existing compute target, using this compute target instead of creating:  ' + cluster_name)\n",
    "    else:\n",
    "        print(\"Error: A compute target with name \",cluster_name,\" was found, but it is not of type AmlCompute.\")\n",
    "else:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size, \n",
    "                                                                min_nodes = cluster_min_nodes, \n",
    "                                                                max_nodes = cluster_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current BatchAI cluster status, use the 'status' property    \n",
    "    print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the default datastore\n",
    "\n",
    "In Lab 1, we uploaded the training files to the default datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AzureBlob mlopsfatosisma6452541516 azureml-blobstore-ba8ddba8-ba9b-45c0-a53f-08c3c660a28d\n"
     ]
    }
   ],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "print(ds.datastore_type, ds.account_name, ds.container_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the hyperparameter space.\n",
    "\n",
    "The hyperparameter space is a range of values defined for each hyperparameter. `Hyperdrive` supports a number of strategies for the hyperparameter space sampling, including random sampling, grid sampling, and Bayesian sampling.\n",
    "\n",
    "In this lab we are going to utilize grid sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import *\n",
    "\n",
    "ps = GridParameterSampling(\n",
    "    {\n",
    "        '--solver': choice('lbfgs', 'newton-cg'),\n",
    "        '--C': choice(0.001, 0.002, 0.005, 0.01, 0.5, 1, 2, 3)\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an estimator object\n",
    "\n",
    "Note that we are not configuring command line parameters that are defined in the hyperparameter grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': ds.as_mount(),\n",
    "    '--filename': 'banking_train.csv'\n",
    "}\n",
    "\n",
    "est_config = Estimator(source_directory=script_folder,\n",
    "                       script_params=script_params,\n",
    "                       compute_target=compute_target,\n",
    "                       entry_script='train.py',\n",
    "                       conda_packages=['scikit-learn', 'pandas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify early termination policy\n",
    "\n",
    "Terminate poorly performing runs automatically with an early termination policy. Termination reduces wastage of resources and instead uses these resources for exploring other parameter configurations.\n",
    "\n",
    "Azure Machine Learning service supports the following Early Termination Policies.\n",
    "\n",
    "- Bandit policy\n",
    "- Median stopping policy\n",
    "- Truncation selection policy\n",
    "- No termination policy\n",
    "\n",
    "In this lab we are going to utilize **No termination policy**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = NoTerminationPolicy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure experiment\n",
    "\n",
    "Now we are ready to configure a `Hyperdrive` experiment. In addition to the configurations defined in the sections above we are also setting resource allocations constraints, including maximum number of training runs, maximum number of concurrent runs and the primary optimization metric.\n",
    "\n",
    "If you go back to visit the training script, you will notice that it logs cross validation accuracy and recall after every run. As you remember from the Lab 1 we want to minimize the number of false negatives - customers who were wrongly identified as ones with low propencity to buy. We want the model with a high recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "htc = HyperDriveRunConfig(estimator=est_config, \n",
    "                          hyperparameter_sampling=ps,\n",
    "                          policy=policy,\n",
    "                          primary_metric_name=\"val_recall\", \n",
    "                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "                          max_total_runs=16,\n",
    "                          max_concurrent_runs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new experient to capture `hyperdrive` runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'propensity_to_buy_hyperdrive'\n",
    "\n",
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the run\n",
    "\n",
    "Finally,launch the hyperdrive job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>propensity_to_buy_hyperdrive</td><td>propensity_to_buy_hyperdrive_1555508196466</td><td>hyperdrive</td><td>Running</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/051aa254-957d-4431-a6df-6caa8963bdd7/resourceGroups/DSIMLOpsHack/providers/Microsoft.MachineLearningServices/workspaces/MLOpsFatosIsmali/experiments/propensity_to_buy_hyperdrive/runs/propensity_to_buy_hyperdrive_1555508196466\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: propensity_to_buy_hyperdrive,\n",
       "Id: propensity_to_buy_hyperdrive_1555508196466,\n",
       "Type: hyperdrive,\n",
       "Status: Running)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdr = exp.submit(config=htc)\n",
    "hdr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd66871842145ea9431a35e4ccc05bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(hdr).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: propensity_to_buy_hyperdrive_1555508196466\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: propensity_to_buy_hyperdrive_1555508196466\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'propensity_to_buy_hyperdrive_1555508196466',\n",
       " 'target': 'cpu-cluster',\n",
       " 'status': 'Completed',\n",
       " 'endTimeUtc': '2019-04-17T13:43:36.000Z',\n",
       " 'properties': {'primary_metric_config': '{\"name\": \"val_recall\", \"goal\": \"maximize\"}',\n",
       "  'runTemplate': 'HyperDrive',\n",
       "  'azureml.runsource': 'hyperdrive',\n",
       "  'ContentSnapshotId': '0e9605ae-dcb1-44be-9792-612db1c86e7d'},\n",
       " 'logFiles': {'azureml-logs/hyperdrive.txt': 'https://mlopsfatosisma6452541516.blob.core.windows.net/azureml/ExperimentRun/dcid.propensity_to_buy_hyperdrive_1555508196466/azureml-logs/hyperdrive.txt?sv=2018-03-28&sr=b&sig=FbFmzQ05UI8MnansWG%2Ft%2BHzcX2f5rPFuxI4oXDQzxYo%3D&st=2019-04-17T13%3A36%3A36Z&se=2019-04-17T21%3A46%3A36Z&sp=r'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdr.wait_for_completion(show_output=True) # specify True for a verbose log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and retrieve the best model\n",
    "\n",
    "When all the jobs finish, you can find the one that hsa the highest performance metrics - in our case *recall*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = hdr.get_best_run_by_primary_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Run Id:  propensity_to_buy_hyperdrive_1555508196466_0\n",
      "\n",
      " Validation recall: 0.6400870938758948\n",
      "\n",
      " solver: lbfgs\n",
      "\n",
      " C: 0.001\n"
     ]
    }
   ],
   "source": [
    "best_run_metrics = best_run.get_metrics()\n",
    "parameter_values = best_run.get_details()['runDefinition']['arguments']\n",
    "\n",
    "print('Best Run Id: ', best_run.id)\n",
    "print('\\n Validation recall:', best_run_metrics['val_recall'])\n",
    "print('\\n solver:',parameter_values[5])\n",
    "print('\\n C:',parameter_values[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the best model\n",
    "\n",
    "The last step in the training script wrote the file model.pkl in the outputs directory. As noted before, outputs is a special directory in that all content in this directory is automatically uploaded to your workspace. This content appears in the run record in the experiment under your workspace.\n",
    "\n",
    "You can register the model so that it can be later queried, examined and deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "propensity_to_buy_predictor\tpropensity_to_buy_predictor:1\t1\n"
     ]
    }
   ],
   "source": [
    "tags = {\"CreatedBy:\": \"HyperDrive\"}\n",
    "model_name = 'propensity_to_buy_predictor'\n",
    "\n",
    "model = best_run.register_model(model_name=model_name, \n",
    "                                model_path='outputs/model.pkl',\n",
    "                                tags=tags)\n",
    "\n",
    "print(model.name, model.id, model.version, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "model_name = 'propensity_to_buy_predictor'\n",
    "model_path = Model.get_model_path(model_name, _workspace=ws)\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.8197378004369993\n",
      "Test recall:  0.6228448275862069\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from azureml.core.model import Model\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Rehydrate the model from Model Registry\n",
    "model_name = 'propensity_to_buy_predictor'\n",
    "model_path = Model.get_model_path(model_name, _workspace=ws)\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "# Load a test dataset\n",
    "folder = '../datasets'\n",
    "filename = 'banking_test.csv'\n",
    "pathname = os.path.join(folder, filename)\n",
    "df = pd.read_csv(pathname, delimiter=',')\n",
    "feature_columns = [\n",
    "                   # Demographic\n",
    "                   'age', \n",
    "                   'job', \n",
    "                   'education', \n",
    "                   'marital',  \n",
    "                   'housing', \n",
    "                   'loan', \n",
    "                   # Previous campaigns\n",
    "                   'month',\n",
    "                   'campaign',\n",
    "                   'poutcome',\n",
    "                   # Economic indicators\n",
    "                   'emp_var_rate',\n",
    "                   'cons_price_idx',\n",
    "                   'cons_conf_idx',\n",
    "                   'euribor3m',\n",
    "                   'nr_employed']\n",
    "df_test = df[feature_columns + ['y']]\n",
    "df_test = pd.get_dummies(df_test, drop_first=True).astype(dtype='float')\n",
    "\n",
    "# Score the test dataset and calculate performance metrics\n",
    "y_pred = model.predict(df_test.drop('y', axis=1))\n",
    "print(\"Test accuracy: \", accuracy_score(df_test.y, y_pred))\n",
    "print(\"Test recall: \", recall_score(df_test.y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "In the next lab, you will operationalize the model developed in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
